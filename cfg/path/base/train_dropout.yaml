env_id: SparseGoalInStatePointsPath-v0
learner_type: Dropout DDPG
render: False
render_eval: False
normalize_returns: False
normalize_observations: False
seed: next
critic_l2_reg: 0.0
tau: 0.01
batch_size: 128  # per MPI worker
actor_lr: 0.0005
critic_lr: 0.005
enable_popart: False
gamma: 0.99
reward_scale: 1
clip_norm: null
noise_type: normal_0.3 # choices are adaptive-param_xx, ou_xx, normal_xx, none
load_path: null

memory_limit: 1000000
nb_train_steps: 100  # per epoch cycle and MPI worker
nb_rollout_steps: 200  # per epoch cycle and MPI worker
num_timesteps: 100000
nb_eval_steps: 200  # per epoch cycle and MPI worker
log_interval: 25
verbose: 1
do_eval: True

use_meta_target: False
teach_behavior_policy: null

dropout_tau: 10.0
include_mc_stats: True

policy_kwargs:
    dropout_keep_prob: 0.8
    mc_samples: 50
    layer_norm: True
    feature_extraction: mlp # Can be mlp or cnn

env_params:
    shuffle_order: True
    dense_reward: False
    render_q_quiver: True

experiment_name: bayesian_ddpg
