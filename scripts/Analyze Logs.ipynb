{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import pylab as pl\n",
    "import matplotlib\n",
    "import time\n",
    "from ggplot import *\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "%matplotlib inline\n",
    "\n",
    "font = {'family' : 'bold',\n",
    "        'size'   : 48}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "matplotlib.rc('legend', loc='lower right')\n",
    "\n",
    "sns.set_context(\"paper\", rc={\"font.size\":48,\n",
    "                             \"font.family\": 'bold',\n",
    "                             \"axes.titlesize\":42,\n",
    "                             \"axes.labelsize\":42, \n",
    "                             \"legend.fontsize\": 24,\n",
    "                             \"legend.loc\": 'lower right',\n",
    "                             \"xtick.labelsize\": 30,\n",
    "                             \"ytick.labelsize\": 30})  \n",
    "pl.rcParams['figure.figsize'] = 15, 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mujoco_dfs = []\n",
    "mujoco_runs = []\n",
    "points_dfs = []\n",
    "points_runs = []\n",
    "hook_dfs = []\n",
    "hook_runs = []\n",
    "hookrandom_dfs = []\n",
    "hookrandom_runs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('points_step_df.pkl'):\n",
    "    points_step_df = pd.read_pickle(\"points_step_df.pkl\")\n",
    "    points_episode_df = pd.read_pickle(\"points_episode_df.pkl\")\n",
    "    points_eval_episode_df = pd.read_pickle(\"points_eval_episode_df.pkl\")\n",
    "    points_dfs = [points_step_df, points_episode_df, points_eval_episode_df]\n",
    "    points_runs = set(list(points_eval_episode_df['run']))\n",
    "    \n",
    "if os.path.isfile('mujoco_step_df.pkl'):\n",
    "    mujoco_step_df = pd.read_pickle(\"mujoco_step_df.pkl\")\n",
    "    mujoco_episode_df = pd.read_pickle(\"mujoco_episode_df.pkl\")\n",
    "    mujoco_eval_episode_df = pd.read_pickle(\"mujoco_eval_episode_df.pkl\")\n",
    "    mujoco_dfs = [mujoco_step_df, mujoco_episode_df, mujoco_eval_episode_df]\n",
    "    mujoco_runs = set(list(mujoco_eval_episode_df['run']))\n",
    "    \n",
    "if os.path.isfile('hook_step_df.pkl'):\n",
    "    hook_step_df = pd.read_pickle(\"hook_step_df.pkl\")\n",
    "    hook_episode_df = pd.read_pickle(\"hook_episode_df.pkl\")\n",
    "    hook_eval_episode_df = pd.read_pickle(\"hook_eval_episode_df.pkl\")\n",
    "    hook_dfs = [hook_step_df, hook_episode_df, hook_eval_episode_df]\n",
    "    hook_runs = set(list(hook_eval_episode_df['run']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_FILES = ['log.log.1','log.log']\n",
    "def parse_logs(log_dirs_path, skip_runs=[], current_dfs=[]):\n",
    "    \n",
    "    def parse_log(log_dir):\n",
    "        step_data = []\n",
    "        episode_data = []\n",
    "        eval_episode_data = []\n",
    "        #for log_dir in os.listdir(log_dirs_path):\n",
    "        cur_log_dir = os.path.join(log_dirs_path,log_dir)\n",
    "        if log_dir in skip_runs:\n",
    "            print(\"Skipping %s\"%cur_log_dir)\n",
    "            return [], [], []\n",
    "        print(\"Parsing %s\"%cur_log_dir)\n",
    "        seed = 0\n",
    "        for seed_dir in os.listdir(cur_log_dir):\n",
    "            seed+=1\n",
    "            for log_file_name in LOG_FILES:\n",
    "                log_file_path = os.path.join(cur_log_dir,seed_dir,log_file_name)\n",
    "                if not os.path.exists(log_file_path):\n",
    "                    continue\n",
    "                with open(log_file_path, 'r') as log_file:\n",
    "                    for line in log_file:\n",
    "                        if 'Starting DDPG training' in line:\n",
    "                            at_episode=0\n",
    "                            at_eval_episode=0\n",
    "                            at_step_total=0\n",
    "                            eval_step=0\n",
    "                            added_eval_episode=False\n",
    "                            eval_training_steps=0\n",
    "                        if 'Starting new episode' in line:   \n",
    "                            at_episode+=1\n",
    "                            at_step = 0\n",
    "                        if 'from policy' in line:\n",
    "                            try:\n",
    "                                policy_choice = int(line.split(' ')[-5])\n",
    "                            except:\n",
    "                                continue\n",
    "                            at_step+=1\n",
    "                            at_step_total+=1\n",
    "                            step_data.append([seed, log_dir, at_episode, at_step, policy_choice])\n",
    "                            added_eval_episode=False\n",
    "                        if 'Episode finished!' in line:\n",
    "                            try:\n",
    "                                reward = float(line.split('=')[-1])\n",
    "                                episode_data.append([seed, log_dir, at_eval_episode, at_step_total, reward])\n",
    "                            except Exception as e:\n",
    "                                logging.error('Issue with parsing line!')\n",
    "                        if 'Eval episode finished' in line:\n",
    "                            if not added_eval_episode:\n",
    "                                eval_training_steps+=200\n",
    "                            reward = float(line.split('=')[-1])\n",
    "                            eval_episode_data.append([seed, log_dir, at_eval_episode, eval_training_steps, reward])\n",
    "                            at_eval_episode+=1\n",
    "                            added_eval_episode=True\n",
    "        return step_data, episode_data, eval_episode_data\n",
    "    \n",
    "    step_datas = []\n",
    "    episode_datas = []\n",
    "    eval_episode_datas = []\n",
    "    for log_dir in os.listdir(log_dirs_path):\n",
    "        step_data, episode_data, eval_episode_data = parse_log(log_dir)\n",
    "        step_datas+=step_data\n",
    "        episode_datas+=episode_data\n",
    "        eval_episode_datas+=eval_episode_data\n",
    "        \n",
    "    step_df = pd.DataFrame(step_datas, columns=['seed', 'run', 'episode', 'step', 'policy_choice'])\n",
    "    episode_df = pd.DataFrame(episode_datas, columns=['seed', 'run', 'episode', 'step', 'episode reward'])\n",
    "    eval_episode_df = pd.DataFrame(eval_episode_datas, columns=['seed', 'run', 'episode', 'training steps', \n",
    "                                                               'episode reward'])\n",
    "    if current_dfs:\n",
    "        step_df = pd.concat([current_dfs[0], step_df])\n",
    "        episode_df = pd.concat([current_dfs[1], episode_df])\n",
    "        eval_episode_df = pd.concat([current_dfs[2], eval_episode_df])\n",
    "        \n",
    "    return step_df,episode_df,eval_episode_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_dfs = parse_logs('SparseGoalInStatePointsPath-v0', points_runs, points_dfs)\n",
    "points_step_df, points_episode_df, points_eval_episode_df = points_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mujoco_dfs = parse_logs('OneGoalPickPlaceDenseEnv-v0', mujoco_runs, mujoco_dfs)\n",
    "#mujoco_dfs = parse_log('OneGoalPickPlaceEnv-v0', mujoco_runs, mujoco_dfs)\n",
    "mujoco_step_df, mujoco_episode_df, mujoco_eval_episode_df = mujoco_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_dfs = parse_logs('FetchHookSweepPushDenseEasyInitEasierTaskCloseInitNoGrasp-v0', hook_runs, hook_dfs)\n",
    "hook_step_df, hook_episode_df, hook_eval_episode_df = hook_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hookrandom_dfs = parse_logs('FetchHookSweepPushDenseEasyInitEasierTaskCloseInitNoGraspRandomized-v0', hookrandom_runs, hookrandom_dfs)\n",
    "hookrandom_step_df, hookrandom_episode_df, hookrandom_eval_episode_df = hookrandom_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_runs = set(list(points_eval_episode_df['run']))\n",
    "points_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mujoco_runs = set(list(mujoco_eval_episode_df['run']))\n",
    "mujoco_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_runs = set(list(hook_eval_episode_df['run']))\n",
    "hook_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_eval_episode_df.loc[hook_eval_episode_df['run']=='efficiency_no_teachers_ddpg','run'] = 'bayesian_ddpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hookrandom_eval_episode_df.loc[hook_eval_episode_df['run']=='efficiency_no_teachers_ddpg','run'] = 'bayesian_ddpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Blurb to check that we have 5 seeds for all the runs we care about... ###\n",
    "\n",
    "POINTS_RUNS_WE_CARE_ABOUT = [\n",
    "    # these are figure 3a (suboptimal + partial teachers)\n",
    "    'efficiency_1full_optimal',   \n",
    "    'efficiency_1full_suboptimal',\n",
    "    'efficiency_4partial_optimal',\n",
    "    'efficiency_4partial_noisy',\n",
    "    # these are figure 3b (incomplete + contradictory teacher sets)\n",
    "    'efficiency_3partial_noisy',\n",
    "    'efficiency_2partial_noisy',\n",
    "    'efficiency_1partial_noisy',\n",
    "    'contradiction_halfway',\n",
    "    # these are figure 3c (larger + lower quality teacher sets)\n",
    "    'sensitivity_bad_teachers_1random',\n",
    "    'sensitivity_bad_teachers_2random',\n",
    "    'sensitivity_bad_teachers_4random',\n",
    "    'sensitivity_bad_teachers_1adversarial',\n",
    "]\n",
    "\n",
    "MUJOCO_RUNS_WE_CARE_ABOUT = [\n",
    "    # (suboptimal + partial teachers)\n",
    "    'efficiency_1full_optimal', \n",
    "    'efficiency_1full_suboptimal',\n",
    "    'efficiency_partial_complete_suboptimal',\n",
    "    'efficiency_partial_complete_optimal',\n",
    "    # (incomplete + contradictory teacher sets)\n",
    "    'efficiency_partial_noisy',\n",
    "    # (TODO: missing place-only teacher sets)\n",
    "    'efficiency_partial_place_noisy',\n",
    "    # (TODO: missing contradictory)\n",
    "    'contradiction_halfway',\n",
    "    # (larger + lower quality teacher sets)\n",
    "    'sensitivity_bad_teachers_1random',\n",
    "    'sensitivity_bad_teachers_2random',\n",
    "    'sensitivity_bad_teachers_4random',\n",
    "    # (TODO: missing adversarial teacher)\n",
    "]\n",
    "\n",
    "#tmp_df = points_eval_episode_df\n",
    "#RUNS_WE_CARE_ABOUT = POINTS_RUNS_WE_CARE_ABOUT\n",
    "\n",
    "tmp_df = mujoco_eval_episode_df\n",
    "RUNS_WE_CARE_ABOUT = MUJOCO_RUNS_WE_CARE_ABOUT\n",
    "\n",
    "tmp_policy_names = ['ours', 'random', 'dqn', 'bayesian_ddpg', 'ddpgcritic']\n",
    "for tmp_run_type in RUNS_WE_CARE_ABOUT:\n",
    "    for tmp_policy_name in tmp_policy_names:\n",
    "        tmp_run = '%s_%s'%(tmp_run_type, tmp_policy_name) if tmp_policy_name!='bayesian_ddpg' else 'bayesian_ddpg'\n",
    "        if 'bayesian_ddpg' in tmp_policy_names:\n",
    "            tmp_sub_df = tmp_df[tmp_df['run'].str.startswith(tmp_run_type) | \n",
    "                        (tmp_df['run']=='bayesian_ddpg') &\n",
    "                        (tmp_df['seed']<=5)].copy()\n",
    "        else:\n",
    "            tmp_sub_df = tmp_df[tmp_df['run'].str.startswith(tmp_run_type) &\n",
    "                        (tmp_df['seed']<=5)].copy()\n",
    "\n",
    "        tmp_sub_sub_df = tmp_sub_df.loc[tmp_sub_df['run']==tmp_run]\n",
    "        try:\n",
    "            max_seed = int(tmp_sub_sub_df['seed'].max())\n",
    "            if max_seed < 5:\n",
    "                print(\"WARNING: Run {} for type {} has {} seeds.\".format(tmp_policy_name, tmp_run_type, max_seed))\n",
    "        except:\n",
    "            print('WARNING: Could not get data for run {} and type {}'.format(tmp_policy_name, tmp_run_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_step_df.to_pickle(\"points_step_df.pkl\")\n",
    "points_episode_df.to_pickle(\"points_episode_df.pkl\")\n",
    "points_eval_episode_df.to_pickle(\"points_eval_episode_df.pkl\")\n",
    "mujoco_step_df.to_pickle(\"mujoco_step_df.pkl\")\n",
    "mujoco_episode_df.to_pickle(\"mujoco_episode_df.pkl\")\n",
    "mujoco_eval_episode_df.to_pickle(\"mujoco_eval_episode_df.pkl\")\n",
    "hook_step_df.to_pickle(\"hook_step_df.pkl\")\n",
    "hook_episode_df.to_pickle(\"hook_episode_df.pkl\")\n",
    "hook_eval_episode_df.to_pickle(\"hook_eval_episode_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounds(df):\n",
    "    optimal_df = df[df['run'] == 'optimal']\n",
    "    if len(optimal_df) == 0:\n",
    "        optimal_df = df[df['run'] == 'run_optimal']\n",
    "    optimal_noisy_df = df[df['run'] == 'optimal_noisy']\n",
    "    if len(optimal_noisy_df) == 0:\n",
    "        optimal_noisy_df = df[df['run'] == 'run_optimal_noisy']\n",
    "    random_df = df[df['run'] == 'random']\n",
    "    optimal_reward = optimal_df['episode reward'].mean()\n",
    "    random_reward = random_df['episode reward'].mean()\n",
    "    teacher_reward = optimal_noisy_df['episode reward'].mean()\n",
    "    return optimal_reward, random_reward, teacher_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plottable_df(df, run_type, \n",
    "                     name_mapping={'ours': 'B-DDPG + ACT (ours)', \n",
    "                                   'dqn': 'B-DDPG + DQN', \n",
    "                                   'random': 'B-DDPG + Random', \n",
    "                                   'ddpgcritic': 'DDPG + Critic',\n",
    "                                   'bayesian_ddpg': 'B-DDPG (no teachers)'}, \n",
    "                     policy_names=['ours', 'random', 'dqn', 'bayesian_ddpg', 'ddpgcritic'], \n",
    "                     smoothing=0.05, print_seed_counts=True):\n",
    "    if 'bayesian_ddpg' in policy_names:\n",
    "        sub_df = df[(df['run'].str.startswith(run_type) | \n",
    "                    (df['run']=='bayesian_ddpg')) &\n",
    "                    (df['seed']<=5)].copy()\n",
    "    else:\n",
    "        sub_df = df[df['run'].str.startswith(run_type) &\n",
    "                    (df['seed']<=5)].copy()\n",
    "\n",
    "    df['smooth rewards'] = 0\n",
    "    for policy_name in policy_names:\n",
    "        run = '%s_%s'%(run_type, policy_name) if policy_name!='bayesian_ddpg' else 'bayesian_ddpg'\n",
    "        sub_sub_df = sub_df.loc[sub_df['run']==run]\n",
    "        try:\n",
    "            max_seed = int(sub_sub_df['seed'].max())\n",
    "        except:\n",
    "            print('Could not get data for policy %s'%policy_name)\n",
    "            continue\n",
    "        if print_seed_counts:\n",
    "            print('Run %s has %d seeds'%(run,max_seed))\n",
    "            \n",
    "        if smoothing:\n",
    "            for seed in range(1, max_seed+1):\n",
    "                if sub_df.loc[sub_df['seed']==seed]['episode reward'].mean()==0:\n",
    "                    sub_df = sub_df.loc[sub_df['seed']!=seed]\n",
    "                else:\n",
    "                    sub_sub_sub_df = sub_sub_df.loc[sub_sub_df['seed']==seed]\n",
    "                    try:\n",
    "                        sub_df.loc[(sub_df['run']==run) & (sub_df['seed']==seed) ,'smooth rewards'] = \\\n",
    "                            sub_sub_sub_df['episode reward'].rolling(int(smoothing*len(sub_sub_sub_df)),1).mean()\n",
    "                    except:\n",
    "                        sub_df = sub_df.loc[sub_df['seed']!=seed]\n",
    "\n",
    "        try: \n",
    "            sub_df.loc[sub_df['run']==run,'run'] = name_mapping[policy_name]\n",
    "        except:\n",
    "            pass\n",
    "    return sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_df(df, x='training steps', y='smooth rewards', \n",
    "            optimal=None, \n",
    "            teacher_optimal=None,\n",
    "            random=None,\n",
    "            ordered_labels=['B-DDPG + ACT (ours)',\n",
    "                            'DDPG + Critic',\n",
    "                            'B-DDPG + DQN',\n",
    "                            'B-DDPG + Random',\n",
    "                            'B-DDPG (no teachers)',\n",
    "                            'DDPG'],\n",
    "            palette={'B-DDPG + ACT (ours)': 'royalblue', \n",
    "                 'B-DDPG + DQN': 'orange', \n",
    "                 'B-DDPG + Random': 'g', \n",
    "                 'DDPG + Critic': 'r',\n",
    "                 'B-DDPG (no teachers)': 'darkmagenta'},\n",
    "            xlim=100000,\n",
    "            ylim=None,\n",
    "            title=None,\n",
    "            name=None,\n",
    "            legend=True):\n",
    "    if not legend:\n",
    "        plot = sns.lineplot(x, y, hue='run', data=df, palette=palette, lw=1, legend=False)\n",
    "    else:\n",
    "        plot = sns.lineplot(x, y, hue='run', data=df, palette=palette, lw=1)\n",
    "    # plot = sns.lineplot(x, y, hue='run', data=df, palette=palette)\n",
    "    # plot = sns.lineplot(x, y, hue='run', data=df, ci='sd')\n",
    "    axes = plot.axes\n",
    "    if title:\n",
    "        axes.set_title(title)\n",
    "    handles, labels = axes.get_legend_handles_labels()\n",
    "    for label in list(ordered_labels):\n",
    "        if label not in labels or label == 'run':\n",
    "            ordered_labels.remove(label)\n",
    "    ordered_handles = []\n",
    "    for label in ordered_labels:\n",
    "        idx = labels.index(label)\n",
    "        if idx!=-1:\n",
    "            ordered_handles.append(handles[idx])\n",
    "    plt.xlim(0,xlim)\n",
    "    if ylim:\n",
    "        plt.ylim(ylim[0],ylim[1])\n",
    "    plot.set_xlabel(\"Training Steps\",fontsize=28)\n",
    "    plot.set_ylabel(\"Discounted Sum of Rewards\",fontsize=28)\n",
    "#     plot.set_ylabel(\"\",fontsize=28)\n",
    "    if optimal: \n",
    "        h = plt.axhline(y=optimal, color='black', linestyle='--', linewidth=2.5)\n",
    "        # plt.text(x=xlim+20, y=optimal, s=\"Optimal\", fontsize=14)  \n",
    "        ordered_handles.append(h)\n",
    "        ordered_labels.append('Full Teacher')\n",
    "    if teacher_optimal: \n",
    "        h = plt.axhline(y=teacher_optimal, color='black', linestyle=':', linewidth=2.5)\n",
    "        # plt.text(x=xlim+20, y=teacher_optimal, s=\"Teachers\", fontsize=14)   \n",
    "        ordered_handles.append(h)\n",
    "        ordered_labels.append('Full Teacher + Noise')\n",
    "    if random: \n",
    "        h = plt.axhline(y=random, color='black', linestyle=':', linewidth=2.5)\n",
    "        # plt.text(x=xlim+20, y=random, s=\"Random\", fontsize=14) \n",
    "        ordered_handles.append(h)\n",
    "        ordered_labels.append('Random Agent')\n",
    "        \n",
    "    if legend:\n",
    "        leg = axes.legend(handles=ordered_handles, \n",
    "                          labels=ordered_labels, \n",
    "                          bbox_to_anchor=(0., 1.02, 1., .102), \n",
    "                          loc=4,#3\n",
    "                          ncol=3,#len(ordered_handles)\n",
    "                         )\n",
    "        for line in leg.get_lines():\n",
    "            line.set_linewidth(5.0)\n",
    "    \n",
    "    if name is not None:\n",
    "        plt.savefig(name, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name_mapping = {'ours': 'B-DDPG + ACT (ours)', \n",
    "                'dqn': 'B-DDPG + DQN', \n",
    "                'random': 'B-DDPG + Random', \n",
    "                'ddpgcritic': 'DDPG + Critic',\n",
    "                'bayesian_ddpg': 'B-DDPG (no teachers)'}\n",
    "\n",
    "policy_names = ['ours', 'random', 'dqn', 'bayesian_ddpg', 'ddpgcritic', ]\n",
    "\n",
    "ordered_labels = ['B-DDPG + ACT (ours)',\n",
    "                      'B-DDPG + DQN',\n",
    "                      'B-DDPG + Random',\n",
    "                      'DDPG + Critic',\n",
    "                      'B-DDPG (no teachers)']\n",
    "\n",
    "palette = {'B-DDPG + ACT (ours)': 'royalblue', \n",
    "           'B-DDPG + ACT (ours, no commitment)': 'brown',\n",
    "           'B-DDPG + DQN': 'orange', \n",
    "           'B-DDPG + Random': 'g', \n",
    "           'DDPG + Critic': 'r',\n",
    "           'B-DDPG (no teachers)': 'darkmagenta'}\n",
    "\n",
    "def plot(r, df, \n",
    "         bounds_df=None, \n",
    "         legend=False, \n",
    "         name=None, \n",
    "         title=None,\n",
    "         tmp_name_mapping=name_mapping,\n",
    "         tmp_policy_names=policy_names,\n",
    "         tmp_ordered_labels=ordered_labels,\n",
    "         tmp_palette=palette,\n",
    "         x='training steps', \n",
    "         xlim=None,\n",
    "         ylim=None):\n",
    "    optimal_reward = None\n",
    "    random_reward = None\n",
    "    teacher_reward = None\n",
    "    if bounds_df is not None:\n",
    "        optimal_reward, random_reward, teacher_reward = get_bounds(bounds_df)\n",
    "        \n",
    "    plottable = get_plottable_df(df, r, name_mapping=tmp_name_mapping, policy_names=tmp_policy_names)\n",
    "        \n",
    "    if name is not None:\n",
    "        name = 'figs/{}.pdf'.format(name)\n",
    "        \n",
    "    plot_df(plottable, x=x, optimal=optimal_reward, \n",
    "            teacher_optimal=teacher_reward, \n",
    "            name=name, \n",
    "            title=title,\n",
    "            legend=legend, \n",
    "            ordered_labels=tmp_ordered_labels, \n",
    "            palette=tmp_palette,\n",
    "            xlim=xlim,\n",
    "            ylim=ylim)\n",
    "    \n",
    "def plot_path(run_name, plot_legend=False, title=None):\n",
    "    plot(run_name, \n",
    "         points_eval_episode_df, \n",
    "         points_episode_df, \n",
    "         plot_legend, \n",
    "         'path_'+run_name,\n",
    "         title=title,\n",
    "         xlim=100000,\n",
    "         ylim=(-0.1,3.4))\n",
    "    \n",
    "def plot_mujoco(run_name, plot_legend=False, title=None):\n",
    "    plot(run_name, \n",
    "         mujoco_eval_episode_df, \n",
    "         mujoco_episode_df, \n",
    "         plot_legend, \n",
    "         'mujoco_'+run_name,\n",
    "         title=title,\n",
    "         xlim=500000,\n",
    "         ylim=(-16,-4))\n",
    "    \n",
    "def plot_hook(run_name, plot_legend=False, title=None):\n",
    "    plot(run_name, \n",
    "         hook_eval_episode_df, \n",
    "         hook_episode_df, \n",
    "         plot_legend, \n",
    "         'hook_'+run_name,\n",
    "         title=title,\n",
    "         xlim=250000,\n",
    "         ylim=(-30,-5))\n",
    "    \n",
    "def plot_hookrandom(run_name, plot_legend=False, title=None):\n",
    "    plot(run_name, \n",
    "         hookrandom_eval_episode_df, \n",
    "         hookrandom_episode_df, \n",
    "         plot_legend, \n",
    "         'hook_'+run_name,\n",
    "         title=title,\n",
    "         xlim=250000,\n",
    "         ylim=(-30,-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_plots = [\n",
    "             ('efficiency_1full_optimal','1 Full Optimal Teacher'),\n",
    "             ('efficiency_1full_suboptimal','1 Full Suboptimal Teacher'),\n",
    "             ('efficiency_4partial_optimal','4 Sufficient Partial Optimal Teachers'),\n",
    "             ('efficiency_4partial_noisy','4 Sufficient Partial Suboptimal Teachers'),\n",
    "             ('efficiency_3partial_noisy','3 Insufficient Partial Suboptimal Teachers'),\n",
    "             ('efficiency_2partial_noisy','2 Insufficient Partial Suboptimal Teachers'),\n",
    "             ('efficiency_1partial_noisy','1 Insufficient Partial Suboptimal Teacher'),\n",
    "             ('contradiction_halfway','2 Sufficient Partial Contradictory Teachers'),\n",
    "             ('sensitivity_bad_teachers_1random','Sufficient Teacher Set + 1 Random Teacher'),\n",
    "             ('sensitivity_bad_teachers_2random','Sufficient Teacher Set + 2 Random Teachers'),\n",
    "             ('sensitivity_bad_teachers_4random','Sufficient Teacher Set + 4 Random Teachers'),\n",
    "             ('sensitivity_bad_teachers_1adversarial','Sufficient Teacher Set + 1 Adversarial Teacher')\n",
    "             ]\n",
    "\n",
    "for run_name,title in path_plots:\n",
    "    plot_path(run_name, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mujoco_plots = [\n",
    "             #('efficiency_1full_optimal','1 Full Teacher'),\n",
    "             #('efficiency_1full_suboptimal','1 Full Noisy Teacher'),\n",
    "             ('efficiency_partial_complete_optimal','Sufficient Partial Teachers'),\n",
    "             ('efficiency_partial_complete_suboptimal','Sufficient Partial Noisy Teachers'),\n",
    "             ('efficiency_partial_incomplete_optimal','Insufficient Partial Teacher'),\n",
    "             ('efficiency_partial_incomplete_suboptimal','Insufficient Partial Noisy Teacher'),\n",
    "             #('contradiction_halfway','Sufficient Partial Contradictory Teachers'),\n",
    "             #('sensitivity_bad_teachers_1random','1 Added Random Teacher'),\n",
    "             #('sensitivity_bad_teachers_2random','2 Added Random Teachers'),\n",
    "             #('sensitivity_bad_teachers_4random','4 Added Random Teachers')\n",
    "               ]\n",
    "\n",
    "for run_name,title in mujoco_plots:\n",
    "    plot_mujoco(run_name, title=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_plots = [\n",
    "             ('efficiency_full_optimal','1 Full Teacher'),\n",
    "             ('efficiency_full_suboptimal','1 Full Noisy Teacher'),\n",
    "             ('efficiency_partial_sufficient_optimal','Sufficient Partial Teachers'),\n",
    "             ('efficiency_partial_sufficient_suboptimal','Sufficient Partial Noisy Teachers'),\n",
    "             ('efficiency_partial_insufficient_optimal','Insufficient Partial Teacher'),\n",
    "             ('efficiency_partial_insufficient_suboptimal','Insufficient Partial Noisy Teacher'),\n",
    "            ]\n",
    "\n",
    "for run_name,title in hook_plots:\n",
    "    plot_hook(run_name, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_plots = [\n",
    "             ('efficiency_full_optimal','1 Full Teacher'),\n",
    "             ('efficiency_full_suboptimal','1 Full Noisy Teacher'),\n",
    "             ('efficiency_partial_sufficient_optimal','Sufficient Partial Teachers'),\n",
    "             ('efficiency_partial_sufficient_suboptimal','Sufficient Partial Noisy Teachers'),\n",
    "             ('efficiency_partial_insufficient_optimal','Insufficient Partial Teacher'),\n",
    "             ('efficiency_partial_insufficient_suboptimal','Insufficient Partial Noisy Teacher'),\n",
    "            ]\n",
    "\n",
    "for run_name,title in hook_plots:\n",
    "    plot_hookrandom(run_name, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mujoco('efficiency_1full_suboptimal', title='1 Full Suboptimal Teacher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sensitivity_tau(df, legend=False, name=None, x='training steps', xlim=100000):\n",
    "\n",
    "     ### sensitivity to tau ###\n",
    "    name_mapping = {'1': 'ACT (tau = 1)', \n",
    "                         '10': 'ACT (tau = 10) - ours',\n",
    "                         '100': 'ACT (tau = 100)',\n",
    "                         '1000': 'ACT (tau = 1000)',\n",
    "                         '10000': 'ACT (tau = 10000)'}\n",
    "    policy_names = ['1', '10', '100', '1000', '10000']\n",
    "\n",
    "    ordered_labels = ['ACT (tau = 1)',\n",
    "                      'ACT (tau = 10) - ours',\n",
    "                      'ACT (tau = 100)',\n",
    "                      'ACT (tau = 1000)',\n",
    "                      'ACT (tau = 10000)']\n",
    "\n",
    "    palette = {'ACT (tau = 1)': 'royalblue', \n",
    "               'ACT (tau = 10) - ours': 'orange', \n",
    "               'ACT (tau = 100)': 'g', \n",
    "               'ACT (tau = 1000)': 'r',\n",
    "               'ACT (tau = 10000)': 'darkmagenta',\n",
    "               'bayesian_ddpg': 'black'}\n",
    "               \n",
    "    plot('sensitivity_tau', df, \n",
    "     legend=legend, \n",
    "     name=name, \n",
    "     tmp_name_mapping=name_mapping,\n",
    "     tmp_policy_names=policy_names,\n",
    "     tmp_ordered_labels=ordered_labels,\n",
    "     tmp_palette=palette,\n",
    "     x=x, \n",
    "     xlim=xlim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sensitivity_tau(points_eval_episode_df, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sensitivity_thresh(df, legend=False, name=None, x='training steps', xlim=100000):\n",
    "\n",
    "     ### sensitivity to tau ###\n",
    "    name_mapping = {'02': 'ACT (beta = 0.2)', \n",
    "                         '04': 'ACT (beta = 0.4)',\n",
    "                         '05': 'ACT (beta = 0.5)',\n",
    "                         '06': 'ACT (beta = 0.6) - ours',\n",
    "                         '08': 'ACT (beta = 0.8)'}\n",
    "    policy_names = [ '02', '04', '05', '06', '08',]\n",
    "\n",
    "    ordered_labels = ['ACT (beta = 0.2)',\n",
    "                      'ACT (beta = 0.4)',\n",
    "                      'ACT (beta = 0.5)',\n",
    "                      'ACT (beta = 0.6) - ours',\n",
    "                      'ACT (beta = 0.8)']\n",
    "\n",
    "    palette = {'ACT (beta = 0.2)': 'royalblue', \n",
    "               'ACT (beta = 0.4)': 'orange', \n",
    "               'ACT (beta = 0.5)': 'g', \n",
    "               'ACT (beta = 0.6) - ours': 'r',\n",
    "               'ACT (beta = 0.8)': 'darkmagenta',\n",
    "               'bayesian_ddpg': 'black'}\n",
    "               \n",
    "    plot('sensitivity_commit_thresh', df, \n",
    "     legend=legend, \n",
    "     name=name, \n",
    "     tmp_name_mapping=name_mapping,\n",
    "     tmp_policy_names=policy_names,\n",
    "     tmp_ordered_labels=ordered_labels,\n",
    "     tmp_palette=palette,\n",
    "     x=x, \n",
    "     xlim=xlim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_sensitivity_thresh(points_eval_episode_df, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sensitivity_mc(df, legend=False, name=None, x='training steps', xlim=100000):   \n",
    "    name_mapping = {'10': 'ACT (MC# = 10)', \n",
    "                    '20': 'ACT (MC# = 20)',\n",
    "                    '30': 'ACT (MC# = 30)',\n",
    "                    '40': 'ACT (MC# = 40)',\n",
    "                    '50': 'ACT (MC# = 50) - ours'}\n",
    "    policy_names = [ '10', '20', '30', '40', '50']\n",
    "\n",
    "    ordered_labels = ['ACT (MC# = 10)',\n",
    "                      'ACT (MC# = 20)',\n",
    "                      'ACT (MC# = 30)',\n",
    "                      'ACT (MC# = 40)',\n",
    "                      'ACT (MC# = 50) - ours']\n",
    "\n",
    "    palette = {'ACT (MC# = 10)': 'royalblue', \n",
    "               'ACT (MC# = 20)': 'orange', \n",
    "               'ACT (MC# = 30)': 'g', \n",
    "               'ACT (MC# = 40)': 'r',\n",
    "               'ACT (MC# = 50) - ours': 'darkmagenta',\n",
    "               'bayesian_ddpg': 'black'}\n",
    "               \n",
    "    plot('sensitivity_mc', df, \n",
    "     legend=legend, \n",
    "     name=name, \n",
    "     tmp_name_mapping=name_mapping,\n",
    "     tmp_policy_names=policy_names,\n",
    "     tmp_ordered_labels=ordered_labels,\n",
    "     tmp_palette=palette,\n",
    "     x=x, \n",
    "     xlim=xlim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sensitivity_mc(points_eval_episode_df, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sensitivity_decay(df, legend=False, name=None, x='training steps', xlim=100000): \n",
    "    name_mapping = {'9': 'ACT (psi = 0.9)', \n",
    "                    '95': 'ACT (psi = 0.95)',\n",
    "                    '98': 'ACT (psi = 0.98)',\n",
    "                    '99': 'ACT (psi = 0.99) - ours',\n",
    "                    '995': 'ACT (psi = 0.995)'}\n",
    "    policy_names = [ '9', '95', '98', '99', '995']\n",
    "\n",
    "    ordered_labels = ['ACT (psi = 0.9)',\n",
    "                      'ACT (psi = 0.95)',\n",
    "                      'ACT (psi = 0.98)',\n",
    "                      'ACT (psi = 0.99) - ours',\n",
    "                      'ACT (psi = 0.995)']\n",
    "\n",
    "    palette = {'ACT (psi = 0.9)': 'royalblue', \n",
    "               'ACT (psi = 0.95)': 'orange', \n",
    "               'ACT (psi = 0.98)': 'g', \n",
    "               'ACT (psi = 0.99) - ours': 'r',\n",
    "               'ACT (psi = 0.995)': 'darkmagenta',\n",
    "               'bayesian_ddpg': 'black'}\n",
    "               \n",
    "    plot('sensitivity_decay', df, \n",
    "     legend=legend, \n",
    "     name=name, \n",
    "     tmp_name_mapping=name_mapping,\n",
    "     tmp_policy_names=policy_names,\n",
    "     tmp_ordered_labels=ordered_labels,\n",
    "     tmp_palette=palette,\n",
    "     x=x, \n",
    "     xlim=xlim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sensitivity_decay(points_eval_episode_df, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ablation(r, df, bounds_df=None, legend=False, name=None, title=None, x='training steps', xlim=100000):\n",
    "    tmp_name_mapping = {'full': 'ACT (full)', \n",
    "                        'no_commitdecay': 'ACT (no commit decay)',\n",
    "                        'no_commit': 'ACT (no commit)',\n",
    "                        'no_meta': 'ACT (no behavioral target)',\n",
    "                        'no_commit_no_meta': 'ACT (no commit & no behavioral target)',\n",
    "                        'no_commitdecay_no_meta':  'ACT (no commit decay & no behavioral target)'}\n",
    "    tmp_policy_names = ['full', 'no_commitdecay', 'no_commit', 'no_meta', \n",
    "                        'no_commit_no_meta']\n",
    "    tmp_ordered_labels = ['ACT (full)',\n",
    "                          'ACT (no commit decay)',\n",
    "                          'ACT (no commit)',\n",
    "                          'ACT (no behavioral target)',\n",
    "                          'ACT (no commit & no behavioral target)']\n",
    "    tmp_palette = {'ACT (full)': 'royalblue', \n",
    "                   'ACT (no commit decay)': 'orange', \n",
    "                   'ACT (no commit)': 'g', \n",
    "                   'ACT (no behavioral target)': 'r',\n",
    "                   'ACT (no commit & no behavioral target)': 'darkmagenta',\n",
    "                   'bayesian_ddpg': 'black'}\n",
    "\n",
    "    df = get_plottable_df(df, r, \n",
    "                          name_mapping=tmp_name_mapping, policy_names=tmp_policy_names)\n",
    "    if 'incomplete' not in r:\n",
    "         df = df[(~df['run'].str.contains('incomplete'))].copy()\n",
    "    \n",
    "    optimal_reward = None\n",
    "    random_reward = None\n",
    "    teacher_reward = None\n",
    "    if bounds_df is not None:\n",
    "        optimal_reward, random_reward, teacher_reward = get_bounds(bounds_df)\n",
    "        \n",
    "    if name is not None:\n",
    "        name = 'figs/{}.pdf'.format(name)\n",
    "        \n",
    "    plot_df(df, x=x, optimal=optimal_reward, \n",
    "            teacher_optimal=teacher_reward, \n",
    "            title=title,\n",
    "            name=name, \n",
    "            legend=legend, \n",
    "            ordered_labels=tmp_ordered_labels, \n",
    "            palette=tmp_palette,\n",
    "            xlim=xlim)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ablation('ablation',mujoco_eval_episode_df,xlim=500000,name='mujoco_ablation',title=\"Ablation for pick and place task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ablation('ablation',points_eval_episode_df,xlim=100000,name='path_ablation',title=\"Ablation for path following task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ablation('ablation_incomplete',points_eval_episode_df,xlim=100000,name='points_ablation_incomplete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = plot_ablation('path_ablation',points_episode_df,x='step')\n",
    "#(ggplot(df, aes('step', 'smooth rewards', colour='run')) + geom_point())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = mujoco_eval_episode_df[mujoco_eval_episode_df['run'].str.startswith('hyperparam')]\n",
    "(ggplot(sub_df, aes('training steps', 'episode reward', colour='run')) + geom_line())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 'efficiency_4partial_noisy'\n",
    "optimal_reward, random_reward, teacher_reward = get_bounds(points_episode_df)\n",
    "df = get_plottable_df(points_eval_episode_df, r, name_mapping=tmp_name_mapping, policy_names=tmp_policy_names)\n",
    "plot_df(df,legend=False, \n",
    "        ordered_labels=tmp_ordered_labels, \n",
    "        palette=tmp_palette,\n",
    "        optimal=optimal_reward, \n",
    "        teacher_optimal=teacher_reward,\n",
    "        xlim=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal_reward, random_reward, teacher_reward = get_bounds(mujoco_episode_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ggplot(df2, aes('training steps', 'smooth rewards', colour='run')) + geom_point())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step_df[step_df['run']=='train_ours_nocommit'].reset_index().hist('policy_choice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @interact(ep_num=widgets.IntSlider(min=1,max=step_df['episode'].max(),step=1))\n",
    "# def plot(ep_num):\n",
    "#     step_df[step_df['episode']==ep_num].hist('policy_choice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @interact(ep_num=widgets.IntSlider(min=1,max=df['episode'].max(),step=1))\n",
    "# def plot(ep_num):\n",
    "#     step_df[df['episode']==ep_num].plot('step', 'policy_choice')\n",
    "#     matplotlib.pyplot.ylim(bottom=0,top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.pivot_table(df[df['episode']>110][df['episode']<=115].reset_index(),\n",
    "#               index='step', columns='episode', values='policy_choice'\n",
    "#              ).plot(subplots=True, sharex=True, sharey=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
